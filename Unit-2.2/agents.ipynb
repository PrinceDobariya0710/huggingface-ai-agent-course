{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "vdNvxyaU6OIV"
      },
      "source": [
        "# Agents in LlamaIndex\n",
        "\n",
        "This notebook is part of the [Hugging Face Agents Course](https://www.hf.co/learn/agents-course), a free Course from beginner to expert, where you learn to build Agents.\n",
        "\n",
        "![Agents course share](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/communication/share.png)\n",
        "\n",
        "## Let's install the dependencies\n",
        "\n",
        "We will install the dependencies for this unit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "deWIx6ag6OIW",
        "outputId": "01e81aa7-e61d-45e7-d4b5-a24a8ff35b8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.9/253.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install llama-index datasets llama-index-callbacks-arize-phoenix llama-index-vector-stores-chroma llama-index-llms-huggingface-api llama-index-llms-google-genai llama-index-embeddings-google-genai -U -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIRF48DJ6OIX"
      },
      "source": [
        "And, let's log in to Hugging Face to use serverless Inference APIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmxvpCdm6OIX"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "_SEVMa066OIY"
      },
      "source": [
        "## Initialising agents\n",
        "\n",
        "Let's start by initialising an agent. We will use the basic `AgentWorkflow` class to create an agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8UiOfwEy6OIY"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms.google_genai import GoogleGenAI\n",
        "from llama_index.core.agent.workflow import AgentWorkflow, ToolCallResult, AgentStream\n",
        "from google.colab import userdata\n",
        "\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Add two numbers\"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "def subtract(a: int, b: int) -> int:\n",
        "    \"\"\"Subtract two numbers\"\"\"\n",
        "    return a - b\n",
        "\n",
        "\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiply two numbers\"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "def divide(a: int, b: int) -> int:\n",
        "    \"\"\"Divide two numbers\"\"\"\n",
        "    return a / b\n",
        "\n",
        "\n",
        "llm = GoogleGenAI(model=\"gemini-2.0-flash\",api_key=userdata.get(\"GOOGLE_API_KEY\"))\n",
        "\n",
        "agent = AgentWorkflow.from_tools_or_functions(\n",
        "    tools_or_functions=[subtract, multiply, divide, add],\n",
        "    llm=llm,\n",
        "    system_prompt=\"You are a math agent that can add, subtract, multiply, and divide numbers using provided tools.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVgcEnfv6OIZ"
      },
      "source": [
        "Then, we can run the agent and get the response and reasoning behind the tool calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vFu0Slve6OIZ",
        "outputId": "5efd0ace-4315-43ba-82e3-18b4d8f89168",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Called tool:  add {'b': 2, 'a': 2} => 4\n",
            "\n",
            "Called tool:  multiply {'a': 4, 'b': 2} => 8\n",
            "(2 + 2) * 2 is 8."
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AgentOutput(response=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={'tool_calls': []}, blocks=[TextBlock(block_type='text', text='(2 + 2) * 2 is 8.')]), tool_calls=[ToolCallResult(tool_name='add', tool_kwargs={'b': 2, 'a': 2}, tool_id='add', tool_output=ToolOutput(content='4', tool_name='add', raw_input={'args': (), 'kwargs': {'b': 2, 'a': 2}}, raw_output=4, is_error=False), return_direct=False), ToolCallResult(tool_name='multiply', tool_kwargs={'a': 4, 'b': 2}, tool_id='multiply', tool_output=ToolOutput(content='8', tool_name='multiply', raw_input={'args': (), 'kwargs': {'a': 4, 'b': 2}}, raw_output=8, is_error=False), return_direct=False)], raw={'content': {'parts': [{'video_metadata': None, 'thought': None, 'code_execution_result': None, 'executable_code': None, 'file_data': None, 'function_call': None, 'function_response': None, 'inline_data': None, 'text': '2 + 2) * 2 is 8.'}], 'role': 'model'}, 'citation_metadata': None, 'finish_message': None, 'token_count': None, 'avg_logprobs': None, 'finish_reason': <FinishReason.STOP: 'STOP'>, 'grounding_metadata': None, 'index': None, 'logprobs_result': None, 'safety_ratings': None, 'usage_metadata': {'cached_content_token_count': None, 'candidates_token_count': 13, 'prompt_token_count': 76, 'total_token_count': 89}}, current_agent_name='Agent')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "handler = agent.run(\"What is (2 + 2) * 2?\")\n",
        "async for ev in handler.stream_events():\n",
        "    if isinstance(ev, ToolCallResult):\n",
        "        print(\"\")\n",
        "        print(\"Called tool: \", ev.tool_name, ev.tool_kwargs, \"=>\", ev.tool_output)\n",
        "    elif isinstance(ev, AgentStream):  # showing the thought process\n",
        "        print(ev.delta, end=\"\", flush=True)\n",
        "\n",
        "resp = await handler\n",
        "resp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1LvOsbl6OIZ"
      },
      "source": [
        "In a similar fashion, we can pass state and context to the agent.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gBgv55LD6OIZ",
        "outputId": "66352b01-92b9-4237-e2b1-6335b3d75bda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AgentOutput(response=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={'tool_calls': []}, blocks=[TextBlock(block_type='text', text='Your name is Bob.\\n')]), tool_calls=[], raw={'content': {'parts': [{'video_metadata': None, 'thought': None, 'code_execution_result': None, 'executable_code': None, 'file_data': None, 'function_call': None, 'function_response': None, 'inline_data': None, 'text': '.\\n'}], 'role': 'model'}, 'citation_metadata': None, 'finish_message': None, 'token_count': None, 'avg_logprobs': None, 'finish_reason': <FinishReason.STOP: 'STOP'>, 'grounding_metadata': None, 'index': None, 'logprobs_result': None, 'safety_ratings': None, 'usage_metadata': {'cached_content_token_count': None, 'candidates_token_count': 6, 'prompt_token_count': 92, 'total_token_count': 98}}, current_agent_name='Agent')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from llama_index.core.workflow import Context\n",
        "\n",
        "ctx = Context(agent)\n",
        "\n",
        "response = await agent.run(\"My name is Bob.\", ctx=ctx)\n",
        "response = await agent.run(\"What was my name again?\", ctx=ctx)\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvrGXsY86OIa"
      },
      "source": [
        "## Creating RAG Agents with QueryEngineTools\n",
        "\n",
        "Let's now re-use the `QueryEngine` we defined in the [previous unit on tools](/tools.ipynb) and convert it into a `QueryEngineTool`. We will pass it to the `AgentWorkflow` class to create a RAG agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9AvMlV3X6OIa"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.llms.google_genai import GoogleGenAI\n",
        "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "\n",
        "# Create a vector store\n",
        "db = chromadb.PersistentClient(path=\"./alfred_chroma_db\")\n",
        "chroma_collection = db.get_or_create_collection(\"alfred\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "\n",
        "# Create a query engine\n",
        "embed_model = GoogleGenAIEmbedding(model_name=\"models/embedding-001\", task_type=\"retrieval_query\",api_key=userdata.get(\"GOOGLE_API_KEY\"))\n",
        "llm = GoogleGenAI(model=\"gemini-2.0-flash\",api_key=userdata.get(\"GOOGLE_API_KEY\"))\n",
        "index = VectorStoreIndex.from_vector_store(\n",
        "    vector_store=vector_store, embed_model=embed_model\n",
        ")\n",
        "query_engine = index.as_query_engine(llm=llm)\n",
        "query_engine_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=query_engine,\n",
        "    name=\"personas\",\n",
        "    description=\"descriptions for various types of personas\",\n",
        "    return_direct=False,\n",
        ")\n",
        "\n",
        "# Create a RAG agent\n",
        "query_engine_agent = AgentWorkflow.from_tools_or_functions(\n",
        "    tools_or_functions=[query_engine_tool],\n",
        "    llm=llm,\n",
        "    system_prompt=\"You are a helpful assistant that has access to a database containing persona descriptions. \",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8NCPZfI6OIa"
      },
      "source": [
        "And, we can once more get the response and reasoning behind the tool calls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WCXOiSKF6OIb",
        "outputId": "f5ff0d18-a46c-4577-8dd8-fdf69b19b378",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Called tool:  personas {'input': 'science fiction'} => Empty Response\n",
            "I'm sorry, I couldn't find any persona descriptions for 'science fiction' in the database. The response I received was empty."
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AgentOutput(response=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={'tool_calls': []}, blocks=[TextBlock(block_type='text', text=\"I'm sorry, I couldn't find any persona descriptions for 'science fiction' in the database. The response I received was empty.\")]), tool_calls=[ToolCallResult(tool_name='personas', tool_kwargs={'input': 'science fiction'}, tool_id='personas', tool_output=ToolOutput(content='Empty Response', tool_name='personas', raw_input={'input': 'science fiction'}, raw_output=Response(response='Empty Response', source_nodes=[], metadata=None), is_error=False), return_direct=False)], raw={'content': {'parts': [{'video_metadata': None, 'thought': None, 'code_execution_result': None, 'executable_code': None, 'file_data': None, 'function_call': None, 'function_response': None, 'inline_data': None, 'text': ' in the database. The response I received was empty.'}], 'role': 'model'}, 'citation_metadata': None, 'finish_message': None, 'token_count': None, 'avg_logprobs': None, 'finish_reason': <FinishReason.STOP: 'STOP'>, 'grounding_metadata': None, 'index': None, 'logprobs_result': None, 'safety_ratings': None, 'usage_metadata': {'cached_content_token_count': None, 'candidates_token_count': 29, 'prompt_token_count': 52, 'total_token_count': 81}}, current_agent_name='Agent')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "handler = query_engine_agent.run(\n",
        "    \"Search the database for 'science fiction' and return some persona descriptions.\"\n",
        ")\n",
        "async for ev in handler.stream_events():\n",
        "    if isinstance(ev, ToolCallResult):\n",
        "        print(\"\")\n",
        "        print(\"Called tool: \", ev.tool_name, ev.tool_kwargs, \"=>\", ev.tool_output)\n",
        "    elif isinstance(ev, AgentStream):  # showing the thought process\n",
        "        print(ev.delta, end=\"\", flush=True)\n",
        "\n",
        "resp = await handler\n",
        "resp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUbP5GpG6OIb"
      },
      "source": [
        "## Creating multi-agent systems\n",
        "\n",
        "We can also create multi-agent systems by passing multiple agents to the `AgentWorkflow` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sdWGknjK6OIb"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.agent.workflow import (\n",
        "    AgentWorkflow,\n",
        "    ReActAgent,\n",
        ")\n",
        "\n",
        "\n",
        "# Define some tools\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Add two numbers.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "def subtract(a: int, b: int) -> int:\n",
        "    \"\"\"Subtract two numbers.\"\"\"\n",
        "    return a - b\n",
        "\n",
        "\n",
        "# Create agent configs\n",
        "# NOTE: we can use FunctionAgent or ReActAgent here.\n",
        "# FunctionAgent works for LLMs with a function calling API.\n",
        "# ReActAgent works for any LLM.\n",
        "calculator_agent = ReActAgent(\n",
        "    name=\"calculator\",\n",
        "    description=\"Performs basic arithmetic operations\",\n",
        "    system_prompt=\"You are a calculator assistant. Use your tools for any math operation.\",\n",
        "    tools=[add, subtract],\n",
        "    llm=llm,\n",
        ")\n",
        "\n",
        "query_agent = ReActAgent(\n",
        "    name=\"info_lookup\",\n",
        "    description=\"Looks up information about XYZ\",\n",
        "    system_prompt=\"Use your tool to query a RAG system to answer information about XYZ\",\n",
        "    tools=[query_engine_tool],\n",
        "    llm=llm,\n",
        ")\n",
        "\n",
        "# Create and run the workflow\n",
        "agent = AgentWorkflow(agents=[calculator_agent, query_agent], root_agent=\"calculator\")\n",
        "\n",
        "# Run the system\n",
        "handler = agent.run(user_msg=\"Can you add 5 and 3?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Er-46ILr6OIb",
        "outputId": "aa1d822a-26bd-406d-f4e4-a1b9e801d591",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```\n",
            "Thought: The current language of the user is: English. I need to use the add tool to add 5 and 3.\n",
            "Action: add\n",
            "Action Input: {\"a\": 5, \"b\": 3}\n",
            "```\n",
            "Called tool:  add {'a': 5, 'b': 3} => 8\n",
            "Thought: I can answer without using any more tools. I'll use the user's language to answer\n",
            "Answer: 8\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AgentOutput(response=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={'tool_calls': []}, blocks=[TextBlock(block_type='text', text='8')]), tool_calls=[ToolCallResult(tool_name='add', tool_kwargs={'a': 5, 'b': 3}, tool_id='33daf417-f589-4a43-875a-8b4e85b89343', tool_output=ToolOutput(content='8', tool_name='add', raw_input={'args': (), 'kwargs': {'a': 5, 'b': 3}}, raw_output=8, is_error=False), return_direct=False)], raw={'content': {'parts': [{'video_metadata': None, 'thought': None, 'code_execution_result': None, 'executable_code': None, 'file_data': None, 'function_call': None, 'function_response': None, 'inline_data': None, 'text': ' to answer\\nAnswer: 8\\n'}], 'role': 'model'}, 'citation_metadata': None, 'finish_message': None, 'token_count': None, 'avg_logprobs': None, 'finish_reason': <FinishReason.STOP: 'STOP'>, 'grounding_metadata': None, 'index': None, 'logprobs_result': None, 'safety_ratings': None, 'usage_metadata': {'cached_content_token_count': None, 'candidates_token_count': 28, 'prompt_token_count': 799, 'total_token_count': 827}}, current_agent_name='calculator')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "async for ev in handler.stream_events():\n",
        "    if isinstance(ev, ToolCallResult):\n",
        "        print(\"\")\n",
        "        print(\"Called tool: \", ev.tool_name, ev.tool_kwargs, \"=>\", ev.tool_output)\n",
        "    elif isinstance(ev, AgentStream):  # showing the thought process\n",
        "        print(ev.delta, end=\"\", flush=True)\n",
        "\n",
        "resp = await handler\n",
        "resp"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}